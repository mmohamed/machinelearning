{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmohamed/machinelearning/blob/deep/deep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ASJWpf8sb4UK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "pip install imdbpy wget tmdbsimple matplotlib seaborn sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xlj9oQmdbDrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import torchvision\n",
        "import urllib\n",
        "import requests\n",
        "import json\n",
        "import imdb\n",
        "import time\n",
        "import itertools\n",
        "import wget\n",
        "import os\n",
        "import tmdbsimple as tmdb\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import sys\n",
        "import os.path\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import pprint\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "tmdb.API_KEY = '9d82bc45b4569d6608d9fbc809d4c5ac' \n",
        "search = tmdb.Search()\n",
        "\n",
        "\n",
        "def grabPosterTmdb(movie):\n",
        "    poster_folder = 'posters_final/'\n",
        "    if not poster_folder.split('/')[0] in os.listdir('./'):\n",
        "       os.mkdir('./' + poster_folder)\n",
        "    response = search.movie(query=movie)\n",
        "    id = response['results'][0]['id']\n",
        "    movie = tmdb.Movies(id)\n",
        "    posterp = movie.info()['poster_path']\n",
        "    title = movie.info()['original_title']\n",
        "    title = '_'.join(title.split(' '))\n",
        "    title = '_'.join(title.split('/'))\n",
        "    title = '_'.join(title.split(':'))\n",
        "    if os.path.isfile(poster_folder + title + '.jpg'):\n",
        "        return\n",
        "    url = 'http://image.tmdb.org/t/p/original' + posterp\n",
        "    f = open(poster_folder + title + '.jpg', 'wb')\n",
        "    f.write(urllib.request.urlopen(url).read())\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def list2pairs(l):\n",
        "    # itertools.combinations(l,2) makes all pairs of length 2 from list l.\n",
        "    pairs = list(itertools.combinations(l, 2))\n",
        "    # then the one item pairs, as duplicate pairs aren't accounted for by itertools\n",
        "    for i in l:\n",
        "        pairs.append([i, i])\n",
        "    return pairs\n",
        "    \n",
        " \n",
        "def getGenresIds():\n",
        "    if os.path.isfile('genresids.pckl'):\n",
        "        fp = open(\"genresids.pckl\", 'rb')\n",
        "        GenreIDtoName = pickle.load(fp)\n",
        "        fp.close()\n",
        "        return GenreIDtoName\n",
        "    \n",
        "    genres = tmdb.Genres()\n",
        "    \n",
        "    list_of_genres = genres.movie_list()['genres']\n",
        "    \n",
        "    GenreIDtoName = {}\n",
        "    for i in range(len(list_of_genres)):\n",
        "        genre_id = list_of_genres[i]['id']\n",
        "        genre_name = list_of_genres[i]['name']\n",
        "        GenreIDtoName[genre_id] = genre_name\n",
        "    # Add not found \"Foreign genre\"    \n",
        "    GenreIDtoName[10769] = \"Foreign\"  \n",
        "    \n",
        "    fp = open(\"genresids.pckl\", 'wb')\n",
        "    pickle.dump(GenreIDtoName, fp)\n",
        "    fp.close()\n",
        "          \n",
        "    return GenreIDtoName\n",
        "\n",
        "\n",
        "def pull():\n",
        "    if os.path.isfile('movies_for_posters.pckl'):\n",
        "        print('Movies already pulled !')\n",
        "        return\n",
        "    # Loading populare movies by geners\n",
        "    movies = []\n",
        "    baseyear = 2019\n",
        "    \n",
        "    print('Starting pulling movies from TMDB, please wait...')\n",
        "    done_ids = []\n",
        "    allIds = getGenresIds()\n",
        "    for g_id in allIds:\n",
        "        baseyear -= 1\n",
        "        for page in range(1, 2, 1):\n",
        "            time.sleep(0.5)\n",
        "        \n",
        "            url = 'https://api.themoviedb.org/3/discover/movie?api_key=' + tmdb.API_KEY\n",
        "            url += '&language=en-US&sort_by=popularity.desc&year=' + str(baseyear) \n",
        "            url += '&with_genres=' + str(g_id) + '&page=' + str(page)\n",
        "            \n",
        "            try:\n",
        "                data = urllib.request.urlopen(url).read()\n",
        "    \n",
        "                dataDict = json.loads(data)\n",
        "                movies.extend(dataDict['results'])\n",
        "            except Exception as e:\n",
        "                print('Error on loading movies lust page ', page, ' caused by , ', str(e) , ', try again...')\n",
        "                \n",
        "        done_ids.append(str(g_id))\n",
        "    print(\"Pulled movies for genres - \" + ','.join(done_ids))\n",
        "    \n",
        "    fp = open(\"movies_for_posters.pckl\", 'wb')\n",
        "    pickle.dump(movies, fp)\n",
        "    fp.close()\n",
        "    print(\"Movies saved - \" + str(len(movies)))\n",
        "    return movies\n",
        "\n",
        "    \n",
        "def clean():\n",
        "    if not os.path.isfile('movies_for_posters.pckl'):\n",
        "        print('Movies fiel data not found !')\n",
        "        return\n",
        "    print('Starting cleaning movies list')\n",
        "    fp = open(\"movies_for_posters.pckl\", 'rb')\n",
        "    movies = pickle.load(fp)\n",
        "    fp.close()\n",
        "    movie_ids = [m['id'] for m in movies]\n",
        "    print(\"originally we had \", len(movie_ids), \" movies\")\n",
        "    movie_ids = np.unique(movie_ids)\n",
        "    seen_before = []\n",
        "    no_duplicate_movies = []\n",
        "    for i in range(len(movies)):\n",
        "        movie = movies[i]\n",
        "        id = movie['id']\n",
        "        if id in seen_before:\n",
        "            continue\n",
        "        else:\n",
        "            seen_before.append(id)\n",
        "            no_duplicate_movies.append(movie)\n",
        "    print(\"After removing duplicates we have \", len(no_duplicate_movies), \" movies\")\n",
        "    return no_duplicate_movies\n",
        "\n",
        "    \n",
        "def clover(movies):\n",
        "    poster_movies = []\n",
        "    counter = 0\n",
        "    movies_no_poster = []\n",
        "    print(\"Total movies : \", len(movies))\n",
        "    print(\"Started downloading posters...\")\n",
        "    for movie in movies:\n",
        "        if counter % 10 == 0 and counter != 0:\n",
        "            print(counter)\n",
        "        id = movie['id']\n",
        "        title = movie['title']\n",
        "        if counter % 300 == 0 and counter != 0:\n",
        "            print(\"Done with \", counter, \" movies!\")\n",
        "            print(\"Trying to get poster for \", title)\n",
        "        try:\n",
        "            grabPosterTmdb(title)\n",
        "            time.sleep(1)\n",
        "            poster_movies.append(movie)\n",
        "        except Exception as e:\n",
        "            print('Error on getting poster for ', title, ' caused by , ', str(e) , ', try again...')\n",
        "            try:\n",
        "                time.sleep(7)\n",
        "                grabPosterTmdb(title)\n",
        "                poster_movies.append(movie)\n",
        "            except:\n",
        "                movies_no_poster.append(movie)\n",
        "        counter += 1\n",
        "    print(\"Done with all the posters!\")    \n",
        "    f = open('poster_movies.pckl', 'wb')\n",
        "    pickle.dump(poster_movies, f)\n",
        "    f.close()\n",
        "    f = open('no_poster_movies.pckl', 'wb')\n",
        "    pickle.dump(movies_no_poster, f)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def getWithOverwiews(movies):\n",
        "    moviesWithOverviews = []\n",
        "    for i in range(len(movies)):\n",
        "        movie = movies[i]\n",
        "        id = movie['id']\n",
        "        overview = movie['overview']\n",
        "        if len(overview) == 0:\n",
        "            continue\n",
        "        else:\n",
        "            moviesWithOverviews.append(movie)\n",
        "    print(\"After removing movies without overviews we have \", len(moviesWithOverviews), \" movies\")      \n",
        "    return moviesWithOverviews        \n",
        "\n",
        "\n",
        "def getBinarizedVectorOfGenres(movies):\n",
        "    genres = []\n",
        "    for i in range(len(movies)):\n",
        "        genres.append(movies[i]['genre_ids'])\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    return mlb.fit_transform(genres)\n",
        "\n",
        "\n",
        "def getBinarizedVectorOfGenresForOne(movies, refid):\n",
        "    genres = []\n",
        "    for i in range(len(movies)):\n",
        "        val = 0\n",
        "        if refid in movies[i]['genre_ids']: \n",
        "            val = 1\n",
        "        genres.append(val)\n",
        "    mlb = LabelBinarizer()\n",
        "    return mlb.fit_transform(genres)\n",
        "\n",
        "     \n",
        "def getBinarizedVectorOfOverview(movies):\n",
        "    content = []\n",
        "    for i in range(len(movies)):\n",
        "        movie = movies[i]\n",
        "        id = movie['id']\n",
        "        overview = movie['overview']\n",
        "        overview = overview.replace(',', '')\n",
        "        overview = overview.replace('.', '')\n",
        "        content.append(overview)\n",
        "    vectorize = CountVectorizer(max_df=0.95, min_df=0.005)\n",
        "    return vectorize.fit_transform(content)\n",
        "\n",
        "\n",
        "# Standard precision recall metrics\n",
        "def precisionRecall(gt, preds):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for t in gt:\n",
        "        if t in preds:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FN += 1\n",
        "    for p in preds:\n",
        "        if p not in gt:\n",
        "            FP += 1\n",
        "    if TP + FP == 0:\n",
        "        precision = 0\n",
        "    else:\n",
        "        precision = TP / float(TP + FP)\n",
        "    if TP + FN == 0:\n",
        "        recall = 0\n",
        "    else:\n",
        "        recall = TP / float(TP + FN)\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "def calculateMetrics(predictions, GenreIDtoName, testMovies, Movies):\n",
        "    precs = []\n",
        "    recs = []\n",
        "    \n",
        "    for i in range(len(testMovies)):\n",
        "        if i % 1 == 0:\n",
        "            pos = testMovies[i]\n",
        "            test = Movies[pos]\n",
        "            gtids = test['genre_ids']\n",
        "            gt = []\n",
        "            for g in gtids:\n",
        "                gname = GenreIDtoName[g]\n",
        "                gt.append(gname)\n",
        "            a, b = precisionRecall(gt, predictions[i])\n",
        "            precs.append(a)\n",
        "            recs.append(b)\n",
        "            \n",
        "    return precs, recs\n",
        "\n",
        "\n",
        "  \n",
        "def buildData():      \n",
        "    if os.path.isfile('poster_movies.pck'):\n",
        "        print('Data already pulled !')\n",
        "        modeldb = open('poster_movies', 'rb')\n",
        "        model = pickle.load(modeldb)\n",
        "        modeldb.close()\n",
        "        return model\n",
        "    pull()\n",
        "    cleanedMovieList = clean()\n",
        "    clover(cleanedMovieList)\n",
        "    return cleanedMovieList\n",
        "  \n",
        "  \n",
        "buildData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myUAQK4xs_6l",
        "colab_type": "code",
        "outputId": "0a81ed0d-9fed-465f-8843-78d1fe41d59e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "\n",
        "if not os.path.isfile('poster_movies.pckl'):\n",
        "  print('Data file not found')\n",
        "  exit(0)\n",
        "fp = open('poster_movies.pckl', 'rb')\n",
        "posterMovies = pickle.load(fp)\n",
        "fp.close()\n",
        "\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "model = nn.Sequential(*list(vgg16.children())[:-1])\n",
        "model = nn.Sequential(*list(model.children())[:-2])\n",
        "\n",
        "posterFolder = 'posters_final/'\n",
        "\n",
        "posters = [j for j in os.listdir(posterFolder) if j.endswith('.jpg ')]\n",
        "\n",
        "featureList = []\n",
        "genreList = []\n",
        "fileOrder = []\n",
        "\n",
        "print('Starting extracting VGG features for scraped images...')\n",
        "print('Total images = ',len(posters))\n",
        "\n",
        "failedFiles = []\n",
        "succesfulFiles = []\n",
        "i = 0\n",
        "\n",
        "for movie in posterMovies:\n",
        "    i+=1\n",
        "\n",
        "    posterName = '_'.join(movie['original_title'].split(' '))\n",
        "    posterName = '_'.join(posterName.split('/'))\n",
        "    posterName = '_'.join(posterName.split(':'))+'.jpg '\n",
        "    \n",
        "    if posterName in posters:\n",
        "        \n",
        "        imagePath = posterFolder + posterName\n",
        "        try:\n",
        "            image = Image.open(imagePath)\n",
        "            # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
        "            minImageSize = 224  \n",
        "            \n",
        "            transformPipeline = transforms.Compose([transforms.Resize([minImageSize,minImageSize]),\n",
        "                                                     transforms.ToTensor(),\n",
        "                                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                          std=[0.229, 0.224, 0.225])])\n",
        "            x = transformPipeline(image)\n",
        "            \n",
        "            image.close()\n",
        "            \n",
        "            x = x.unsqueeze(0)  # Insert the new axis at index 0 i.e. in front of the other axes/dims. \n",
        "\n",
        "            x = Variable(x)\n",
        "            \n",
        "            succesfulFiles.append(posterName)\n",
        "            \n",
        "            features = model(x)\n",
        "            \n",
        "            fileOrder.append(imagePath)\n",
        "            featureList.append(features)\n",
        "            genreList.append(movie['genre_ids'])\n",
        "            \n",
        "            maxPrediction = features.data.numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
        "            \n",
        "            if maxPrediction == 0.0:\n",
        "                print('problematic ',i)\n",
        "            if i%25 == 0 or i == 1:\n",
        "                print('Working on Image : ',i)\n",
        "        except Exception as e:\n",
        "            failedFiles.append(posterName)\n",
        "            print('Error on transform image ' , posterName ,' caused by , ', str(e) )\n",
        "            continue\n",
        "        \n",
        "    else:\n",
        "        continue\n",
        "    \n",
        "print('Done with all available features ', len(succesfulFiles) , ', please pickle for future use!')\n",
        "\n",
        "listPickled = (featureList, fileOrder, failedFiles,succesfulFiles, genreList)\n",
        "\n",
        "fp = open('posters_new_features.pckl','wb')\n",
        "pickle.dump(listPickled,fp)\n",
        "fp.close()\n",
        "print('Features dumped to pickle file')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting extracting VGG features for scraped images...\n",
            "Total images =  348\n",
            "Working on Image :  1\n",
            "Working on Image :  25\n",
            "Working on Image :  50\n",
            "Working on Image :  75\n",
            "Working on Image :  100\n",
            "Working on Image :  125\n",
            "Working on Image :  150\n",
            "Working on Image :  200\n",
            "Working on Image :  225\n",
            "Working on Image :  250\n",
            "Working on Image :  275\n",
            "Working on Image :  300\n",
            "Working on Image :  325\n",
            "Working on Image :  350\n",
            "Done with all available features  345 , please pickle for future use!\n",
            "Features dumped to pickle file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mAh_LL7zRLbg",
        "colab_type": "code",
        "outputId": "b37f64d6-af50-4c53-b7b4-2af8e4e9e69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "if not os.path.isfile('posters_new_features.pckl'):\n",
        "  print('Features file not found')\n",
        "  exit(0)\n",
        "fp = open('posters_new_features.pckl', 'rb')\n",
        "listPickled = pickle.load(fp)\n",
        "fp.close()\n",
        "\n",
        "(featureList, files, failed, succesful, genreList) = listPickled\n",
        "\n",
        "print(featureList[0].shape)\n",
        "(a,b,c,d) = featureList[0].shape\n",
        "\n",
        "featureSize = a*b*c*d\n",
        "\n",
        "npFeatures = np.zeros((len(featureList),featureSize))\n",
        "\n",
        "print(len(featureList),featureSize)\n",
        "\n",
        "for i in range(len(featureList)):\n",
        "    feat = featureList[i]\n",
        "    reshapedFeat = feat.reshape(1,-1)\n",
        "    npFeatures[i] = reshapedFeat\n",
        "\n",
        "X = npFeatures\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "Y = mlb.fit_transform(genreList)\n",
        "\n",
        "visualProblemData = (X,Y)\n",
        "fp = open('visual_problem_data_clean.pckl','wb')\n",
        "pickle.dump(visualProblemData,fp)\n",
        "fp.close()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "345 150528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EwbQmV88bFLN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import pickle\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import pprint\n",
        "import copy\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "      \n",
        "if not os.path.isfile('visual_problem_data_clean.pckl'):\n",
        "  print('Visual Data file not found')\n",
        "  exit(0)\n",
        "fp = open('visual_problem_data_clean.pckl', 'rb')\n",
        "visualFeatures = pickle.load(fp)\n",
        "fp.close()\n",
        "\n",
        "(X,Y) = visualFeatures\n",
        "\n",
        "mask = np.random.rand(len(X)) < 0.8\n",
        "\n",
        "X_train = X[mask]\n",
        "X_test = X[~mask]\n",
        "Y_train = Y[mask]\n",
        "Y_test = Y[~mask]\n",
        "\n",
        "\n",
        "class FeatureDataset():\n",
        "    def __init__(self, X_train, Y_train, transform=None):\n",
        "    \n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return X_train[idx].astype(float), Y_train[idx].astype(float)\n",
        "      \n",
        "\n",
        "dsets = {}\n",
        "dsets['train'] = FeatureDataset(X_train, Y_train)\n",
        "dsets['test'] = FeatureDataset(X_test, Y_test)\n",
        "\n",
        "dsetLoaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=64, shuffle=True, num_workers=25) for x in ['train', 'test']}\n",
        "\n",
        "dsetSizes = {x: len(dsets[x]) for x in ['train', 'test']}\n",
        "\n",
        "print(dsetSizes)\n",
        "\n",
        "\n",
        "def trainModel(model, criterion, optimizer, numEpochs):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    bestModel = model\n",
        "    bestACC = 0.0\n",
        "    useGPU = 0\n",
        "    \n",
        "    for epoch in range(numEpochs):\n",
        "        print('Epoch {}/{}'.format(epoch, numEpochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                mode = 'train'\n",
        "                model.train()  # Set model to training mode\n",
        "                print(\"TRAINING STARTED\")\n",
        "            else:\n",
        "                model.eval()\n",
        "                mode = 'val'\n",
        "                print(\"TESTING STARTED\")\n",
        "\n",
        "            runningLoss = 0.0\n",
        "            runningCorrects = 0\n",
        "\n",
        "            counter = 0\n",
        "            # Iterate over data.\n",
        "            answer = []\n",
        "            for data in dsetLoaders[phase]:\n",
        "                inputs, labels = data \n",
        "                \n",
        "                # wrap them in Variable\n",
        "                if useGPU:\n",
        "                    try:\n",
        "                        inputs, labels = Variable(inputs.float().cuda()),                             \n",
        "                        Variable(labels.long().cuda())\n",
        "                    except:\n",
        "                        print(inputs,labels)\n",
        "                else:\n",
        "                    inputs, labels = Variable(Variable(inputs).float()), Variable(Variable(labels).float())\n",
        "\n",
        "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                for i in range(len(outputs.data)):\n",
        "                    answer.append(outputs.data[i])\n",
        "                _, preds = torch.max(outputs.data, 1)\n",
        "                \n",
        "                loss = criterion(outputs, labels)\n",
        "                print('loss done')                \n",
        "                # Just so that you can keep track that something's happening and don't feel like the program isn't running.\n",
        "                if counter%50 == 0:\n",
        "                    print(\"Reached iteration \",counter)\n",
        "                counter += 1\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    print('loss backward')\n",
        "                    loss.backward()\n",
        "                    print('done loss backward')\n",
        "                    optimizer.step()\n",
        "                    print('done optimizer')\n",
        "                try:\n",
        "                    runningLoss += float(loss.item())\n",
        "                    # print(preds.shape, labels.data.shape)\n",
        "                    for q in range(len(labels.data)):\n",
        "                        if labels.data[q][preds[q]] == 1:\n",
        "                            runningCorrects += 1\n",
        "                except Exception as e:\n",
        "                    print('unexpected error, could not calculate loss or do a sum, cause by ', str(e))\n",
        "            print('trying epoch loss')\n",
        "            epochLoss = runningLoss / dsetSizes[phase]\n",
        "            epochACC = runningCorrects / dsetSizes[phase]\n",
        "            print(phase, runningCorrects, dsetSizes[phase])\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epochLoss, epochACC))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test':\n",
        "                if epochACC > bestACC:\n",
        "                    bestACC = epochACC\n",
        "                    bestModel = copy.deepcopy(model)\n",
        "                    print('new best accuracy = ',bestACC)\n",
        "\n",
        "    timeElapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(timeElapsed // 60, timeElapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(bestACC))\n",
        "    print('returning and looping back')\n",
        "    return bestModel, answer      \n",
        "\n",
        "\n",
        "modelVisual = nn.Sequential(\n",
        "    nn.Linear(X.shape[1],1024), \n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1024,256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256,Y_test.shape[1]),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.RMSprop(modelVisual.parameters(), lr=0.0001, weight_decay=1e-6)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "modelFt, YPreds = trainModel(modelVisual, criterion, optimizer, numEpochs=50)\n",
        "\n",
        "\n",
        "\n",
        "print('Save model')\n",
        "torch.save(modelFt.state_dict(), 'fine_tuned_best_model.pt')\n",
        "\n",
        "print('Save predected Y')\n",
        "fp = open('Y-predect.pckl','wb')\n",
        "pickle.dump(YPreds, fp)\n",
        "fp.close()\n",
        "\n",
        "print('Save Input-model')\n",
        "fp = open('input-model.pckl','wb')\n",
        "pickle.dump((X_train, Y_train, X_test, Y_test), fp)\n",
        "fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RyH-jpEz7-DL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "578ef4de-d2d1-4f6f-f4ae-676503e018eb"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Standard precision recall metrics\n",
        "def precisionRecall(gt, preds):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for t in gt:\n",
        "        if t in preds:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FN += 1\n",
        "    for p in preds:\n",
        "        if p not in gt:\n",
        "            FP += 1\n",
        "    if TP + FP == 0:\n",
        "        precision = 0\n",
        "    else:\n",
        "        precision = TP / float(TP + FP)\n",
        "    if TP + FN == 0:\n",
        "        recall = 0\n",
        "    else:\n",
        "        recall = TP / float(TP + FN)\n",
        "    return precision, recall\n",
        "  \n",
        "  \n",
        "bestModel = torch.load('fine_tuned_best_model.pt')\n",
        "\n",
        "if not os.path.isfile('genresids.pckl'):\n",
        "  print('Genres ID Data file not found')\n",
        "  exit(0)\n",
        " \n",
        "fp = open('genresids.pckl','rb')\n",
        "GenreIDtoName = pickle.load(fp)\n",
        "fp.close()\n",
        "\n",
        "genreList = sorted(list(GenreIDtoName.keys()))\n",
        "\n",
        "if not os.path.isfile('input-model.pckl'):\n",
        "  print('Input model Data file not found')\n",
        "  exit(0)\n",
        " \n",
        "fp = open('input-model.pckl','rb')\n",
        "Xtrain, Ytrain, Xtest, Ytest = pickle.load(fp)\n",
        "fp.close()\n",
        "\n",
        "if not os.path.isfile('Y-predect.pckl'):\n",
        "  print('Predected Y Data file not found')\n",
        "  exit(0)\n",
        " \n",
        "fp = open('Y-predect.pckl','rb')\n",
        "Ypreds = pickle.load(fp)\n",
        "fp.close()\n",
        "\n",
        "precs = []\n",
        "recs = []\n",
        "\n",
        "for i in range(len(Ypreds)):\n",
        "    row = Ypreds[i]\n",
        "    gtGenres = Ytest[i]\n",
        "    gtGenreNames = []\n",
        "    \n",
        "    for j in range(len(gtGenres)):\n",
        "        if gtGenres[j]==1:\n",
        "            gtGenreNames.append(GenreIDtoName[genreList[j]])\n",
        "            \n",
        "    top = np.argsort(row)[-3:]\n",
        "    predictedGenres=[]\n",
        "    \n",
        "    for genre in top:\n",
        "        predictedGenres.append(GenreIDtoName[genreList[genre]])\n",
        "        \n",
        "    precision,recall = precisionRecall(gtGenreNames, predictedGenres)\n",
        "    \n",
        "    precs.append(precision)\n",
        "    recs.append(recall)\n",
        "    \n",
        "    if i%50 == 0:\n",
        "        print('Predicted: ',', '.join(predictedGenres),' Actual: ',', '.join(gtGenreNames))\n",
        "        \n",
        "print('Deep Precision-Recall : ')\n",
        "print('Precision AVG: ', np.mean(np.asarray(precs)), 'Recall AVG:', np.mean(np.asarray(recs)))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted:  Action, Adventure, Fantasy  Actual:  Adventure, Animation, Action, Comedy, Science Fiction\n",
            "Predicted:  Romance, Family, Animation  Actual:  Thriller, Mystery\n",
            "Deep Precision-Recall : \n",
            "Precision AVG:  0.23943661971830982 Recall AVG: 0.20704225352112676\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}